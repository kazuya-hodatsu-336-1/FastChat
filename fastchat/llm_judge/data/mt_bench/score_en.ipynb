{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b3592ec6-d74b-4b84-a951-340ffc94c80c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['Qwen3-4B_t0.2' 'HCSD_MPO_v2_Qwen3-4B_t0.2' 'Qwen3-4B-HCSD_MPO_SFT3_t02'\n",
      " 'Qwen3-4B-HCSD_MPO_SFT1_16k_t02' 'Qwen3-4B-HCSD_MPO_SFT1_4k_t02_1.2'\n",
      " 'HCSD_MPO_v2_Qwen3-8B' 'HCSD_MPO_v2_Qwen3-8B_16k' 'Qwen3-8B_16k'\n",
      " 'HCSD_MPO_v2_Qwen3-8B_t00' 'HCSD_MPO_v2_Qwen3-8B_0.2_0306'\n",
      " 'HCSD_MPO_v2_Qwen3-8B_0.2_0318' 'HCSD_MPO_v2_Qwen3-8B_0.6_0301'\n",
      " 'HCSD_MPO_v2_Qwen3-8B_0.6_0312' 'Qwen3-8B_0.2_0346' 'Qwen3-8B_0.2_0355'\n",
      " 'Qwen3-8B_0.6_0341' 'Qwen3-8B_0.6_0351']\n",
      "Model: Qwen3-4B_t0.2, Score: 8.49\n",
      "Model: HCSD_MPO_v2_Qwen3-4B_t0.2, Score: 8.52\n",
      "Model: Qwen3-4B-HCSD_MPO_SFT3_t02, Score: 8.47\n",
      "Model: Qwen3-4B-HCSD_MPO_SFT1_16k_t02, Score: 8.46\n",
      "Model: Qwen3-4B-HCSD_MPO_SFT1_4k_t02_1.2, Score: 8.22\n",
      "Model: HCSD_MPO_v2_Qwen3-8B, Score: 8.27\n",
      "Model: HCSD_MPO_v2_Qwen3-8B_16k, Score: 8.70\n",
      "Model: Qwen3-8B_16k, Score: 8.49\n",
      "Model: HCSD_MPO_v2_Qwen3-8B_t00, Score: 7.64\n",
      "Model: HCSD_MPO_v2_Qwen3-8B_0.2_0306, Score: 8.49\n",
      "Model: HCSD_MPO_v2_Qwen3-8B_0.2_0318, Score: 8.64\n",
      "Model: HCSD_MPO_v2_Qwen3-8B_0.6_0301, Score: 8.56\n",
      "Model: HCSD_MPO_v2_Qwen3-8B_0.6_0312, Score: 8.70\n",
      "Model: Qwen3-8B_0.2_0346, Score: 8.55\n",
      "Model: Qwen3-8B_0.2_0355, Score: 8.57\n",
      "Model: Qwen3-8B_0.6_0341, Score: 8.72\n",
      "Model: Qwen3-8B_0.6_0351, Score: 8.73\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# CATEGORIES = [\"Coding\", \"Extraction\", \"Humanities\", \"Math\", \"Reasoning\", \"Roleplay\", \"STEM\", \"Writing\"]\n",
    "\n",
    "def get_model_df():\n",
    "    q2result = []\n",
    "    with open(\"./model_judgment/gpt-4o_single.jsonl\", \"r\") as fin:\n",
    "        for line in fin:\n",
    "            obj = json.loads(line)\n",
    "            # obj[\"category\"] = CATEGORIES[(int(obj[\"question_id\"])-1)//10]\n",
    "            q2result.append(obj)\n",
    "    return pd.DataFrame(q2result)\n",
    "\n",
    "df = get_model_df()\n",
    "all_models = df[\"model\"].unique()\n",
    "print(\"Available models:\", all_models)\n",
    "##\n",
    "scores_all = []\n",
    "for model in all_models:\n",
    "    # for cat in CATEGORIES:\n",
    "    res = df[(df[\"model\"]==model) & (df[\"score\"] >= 0)]\n",
    "    score = res[\"score\"].mean()\n",
    "    scores_all.append({\"model\": model, \"score\": score})\n",
    "\n",
    "target_models = all_models\n",
    "scores_target = [score for score in scores_all if score[\"model\"] in target_models]\n",
    "\n",
    "# スコアを表示\n",
    "for score in scores_target:\n",
    "    print(f\"Model: {score['model']}, Score: {score['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
